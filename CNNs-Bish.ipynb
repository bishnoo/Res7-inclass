{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YkBXfjuHjj5O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "4zUSk6gHjj5W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "metadata": {
        "id": "yvV4UE3fjj5b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "metadata": {
        "id": "udYSGrYDjj5h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QeBpouObjj5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "423b3c9b-425c-4128-b37a-e573c6339da9"
      },
      "cell_type": "code",
      "source": [
        "##Example: Loading MNIST dataset\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eqE3Njewjj5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "metadata": {
        "id": "46ceMWe3jj51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb5d1fa5-a65a-4972-bebf-aaa52951dffa"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "SfpHdBgCjj6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e98e78a-ae68-4e0c-8502-eee1bc70c966"
      },
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "kVVI8jnejj6O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "metadata": {
        "id": "MDxtDEaMjj6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b88cdaf0-4d72-40b3-b78c-4f0ecc8bd0e0"
      },
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "JV3hERldjj6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "metadata": {
        "id": "6aFpQO7ijj6c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train=tf.keras.utils.to_categorical(y_train)\n",
        "y_test=tf.keras.utils.to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ByCjpWREjj6k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "imhgI9nAjj6r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "metadata": {
        "id": "WoYnEzhCjj6u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train/255\n",
        "x_test = x_test/255\n",
        "y_train = y_train/255\n",
        "y_test = y_test/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2KtVctpEjj61",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "metadata": {
        "id": "Wr-R9ueDjj65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yYLQERXejj7A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "metadata": {
        "id": "7psAELrgjj7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import cifar10, mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.rcParams['figure.figsize'] = (15, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ttMbp178jj7N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=30`. **"
      ]
    },
    {
      "metadata": {
        "id": "0zRbbYlElyGk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN = True\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jXNk-BNtjj7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "440836af-352d-4e4e-e64d-53c10ff232d5"
      },
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "    # Define model\n",
        "    model1 = Sequential()\n",
        "\n",
        "    # 1st Conv Layer\n",
        "    model1.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model1.add(Convolution2D(32, 3, 3))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model1.add(Flatten())\n",
        "    model1.add(Dense(128))\n",
        "    model1.add(Activation('relu'))\n",
        "\n",
        "    # Prediction Layer\n",
        "    model1.add(Dense(10))\n",
        "    model1.add(Activation('softmax'))\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Store Training Results\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n",
        "\n",
        "    # Train the model2\n",
        "    model1.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=30, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)\n",
        "    \n",
        "    # Save model2 and Stats\n",
        "    model1.save('./mnist_basic_cnn.h5')\n",
        "\n",
        "else:\n",
        "    print('Loading pretrained model...')\n",
        "    model2 = keras.models.load_model('./mnist_basic_cnn.h5')\n",
        "    print('Model Loaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 30s 492us/step - loss: 0.0015 - acc: 0.8637 - val_loss: 0.0011 - val_acc: 0.8946\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 25s 418us/step - loss: 9.1862e-04 - acc: 0.9133 - val_loss: 0.0010 - val_acc: 0.9039\n",
            "Epoch 3/30\n",
            "54368/60000 [==========================>...] - ETA: 2s - loss: 6.6402e-04 - acc: 0.9366"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Eiw37t1gjj7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "metadata": {
        "id": "FwEJwLo0jj7b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "    # Deine Model\n",
        "    model3 = Sequential()\n",
        "\n",
        "    # 1st Conv Layer\n",
        "    model3.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n",
        "    model3.add(Activation('relu'))\n",
        "\n",
        "    # 2nd Conv Layer\n",
        "    model3.add(Convolution2D(32, 3, 3))\n",
        "    model3.add(Activation('relu'))\n",
        "\n",
        "    # Max Pooling\n",
        "    model3.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    # Dropout\n",
        "    model3.add(Dropout(0.25))\n",
        "\n",
        "    # Fully Connected Layer\n",
        "    model3.add(Flatten())\n",
        "    model3.add(Dense(128))\n",
        "    model3.add(Activation('relu'))\n",
        "    \n",
        "    # More Dropout\n",
        "    model3.add(Dropout(0.5))\n",
        "\n",
        "    # Prediction Layer\n",
        "    model3.add(Dense(10))\n",
        "    model3.add(Activation('softmax'))\n",
        "\n",
        "    # Loss and Optimizer\n",
        "    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # Store Training Results\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=7, verbose=1, mode='auto')\n",
        "    callback_list = [early_stopping]\n",
        "\n",
        "    # Train the model\n",
        "    model3.fit(x_train, y_train, batch_size=BATCH_SIZE, nb_epoch=EPOCHS, \n",
        "              validation_data=(x_test, y_test), callbacks=callback_list)\n",
        "    \n",
        "    # Save model and Stats\n",
        "    model3.save('./drive/models/cnn.h5')\n",
        "\n",
        "else:\n",
        "    print('Loading pretrained model...')\n",
        "    model3 = keras.models.load_model('./drive/cnn.h5')\n",
        "    print('Model Loaded.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55sSZY6Zjj7e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "metadata": {
        "id": "rO3VivC0jj7f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "metadata": {
        "id": "L7C1ed6Fjj7h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jpze5yM7jj7m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "2fLtYKXpjj7p",
        "colab_type": "code",
        "colab": {},
        "outputId": "c2bceb5e-1ec3-401b-aad2-d02ce34c697f"
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABcCAYAAAB+6068AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5491602650>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "ouXVvEppjj7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "metadata": {
        "id": "US-zzM7Gjj7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJK99Ltqjj72",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "metadata": {
        "id": "8WhnDFtYjj73",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}